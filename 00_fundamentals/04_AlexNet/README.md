LeNet处理的MNIST还是比较简单，像素少，通道单一，预测值也少，当我们需要判别更大更精细的图像时LeNet模型的能力就不够了，这个时候AlexNet出现了，AlexNet论文中是使用imagenet作为数据集的，我们这里使用的是STL-10，大小2个g，不大不小，训练时间在1个小时左右,对于我使用的4060显卡。

在代码里我们直接用的alex的模型函数，具体的模型构造方式如下

227x227x3图像
conv1+ReLU
    conv1 3通道 96卷积 11x11 步长4 边缘2填充
        我们用很大的步长很大的卷积核来抽取特征，是因为这里图像很大，我们先大致地捕捉大的特征，减少后续计算量。
    ReLU
        LeNet用tanh作为激活函数，在输入很大或很小时，导数会接近0，反向传播时，梯度会容易消失，在层数大时，这个情况尤其严重，而ReLU通过简单地保留大于0的输出来非线性激活，这样输入大于0，导数为1，反向传播的时候梯度被完整传到前层
->55x55x96

MaxPool1 
    池化核3x3 步长2
    之所以强调max是和前面LeNet的平均池化对比，我们只取选择区域最大值来池化，很明显最大池化能强化特征，平均池化模糊特征，不过同时，平均池化能照顾整体情况，但很显然我们在特征繁多的复杂图像里，我们更需要强化特征，而不是模糊特征，即使这会一定程度丢失对整体的把握
-> 27×27×96

Conv2
    96通道 256卷积核 5x5 步长2 边缘1填充
    这里我们96个通道和前面的96个输出，这种全连接的方式能够整合R G B三个通道下的相同特征，帮助模型更好的学习颜色和形状的关系。
-> 27x27x256

MaxPool2
    池化核3x3 步长2
-> 13x13x256

conv3
    256通道 384卷积核 3x3 步长1 边缘1填充
-> 13x13x384

conv4
    384通道 384卷积核 3x3 步长1 边缘1填充
-> 13x13x384

conv5
    384通道 256卷积核 3x3 步长1 边缘1填充
-> 13x13x256

    这里开始我们用的全是3x3的卷积，并且减小了步长，区别于前面的卷积，我们在这里开始捕捉细小特征，并且通过连续卷积中先提取特征再不断地组合特征，通过这种特征的组合加强对图像整体的理解。还能提高感受野（不懂找豆包doge）

    思考！！！
    1，为什么中间不池化 
        因为会丢失特征细节，这样连续卷积后（特征的组合）过后因为细节的缺失不能组成准确的特征组合
    2，为什么不多卷积几层 
        1这就涉及到深层卷积了，我们这里相对lenet已经增加了卷积层数，得益于GPU和relu激活函数，但是继续卷积还是很容易造成梯度消失和爆炸的问题，下一篇ResNet我会学习相关知识来继续怎么层数。
        2卷积层数过多而且会过拟合，导致泛化，模型过度学习细节导致效果变差。
        3太难算了，程序要训练很久，而且模型够用就行
-> 13x13x256

MaxPool3
    池化核3x3 步长2
->6x6x256=9216
最后一次池化后续交给全连接层

全连接层
FC6
    4096个神经元
    ReLU激活并且Dropout=0.5 随机让一半神经元休眠
FC7
    4096个神经元
    ReLU激活并且Dropout=0.5 随机让一半神经元休眠

这里dropout作用直观且强大，通过让随机神经元休眠，让模型不依赖部分神经元，防止过拟合

输出层
    1000个神经元
    这个和lenet一样算分数，再用softmax算概率

最后！！！目前的模型跑出来只有60的正确率，我估计主要原因是alexnet的模型用在imagenet上很合适，但是用在stl10上面还是不够合适，后续会想办法提高一下目标到75上下，提高多少算多少，但是先把ResNet学习完然后认真沉淀一下把前面的知识再学习一下，改一改代码，观察改变模型结构后，结果的变化，但当前重要的还是下一次会议的汇报。
        