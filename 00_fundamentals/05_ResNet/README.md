代码部分基本套用AlexNet，就修改了模型的定义，但其实还是用的系统自带的ResNet的模型
并且！我们直接用迁移学习，也就是让我们的ResNet继承ImageNet预训练得到的权重（数据集依旧是stl10，imageNet太大了下不动doge）在第一个epoch就获得了很好的结果也就是90的正确率，体现了ResNet的可复用性和鲁棒性

ResNet解决的随着网络变深，准确率却下降的问题
核心思想 残差连接
    残差连接重在残差，也就是学习的是fx=hx-x 而不是直接学习 hx
    好处：  反向传播梯度最小为1，缓解梯度消失
            输入不变时fx=0 而不是x 更容易
    
    实现方式：残差块
        基本块 浅层18，34
        输入->(3x3卷积->BN+relu)->(3x3卷积->BN)->跳跃连接->relu->输出
        这里BN是批归一化 跳跃连接实现残差化

        瓶颈块 深层50，101，152
        输入->(1x1卷积->BN+relu降维)->(3x3卷积->BN+relu)->(1x1卷积->BN升维)->跳跃连接->relu->输出
        这里降维升维指的是改变输出通道，让输出通道是输入通道的多少倍，这里降维一般是1/4升维是4倍，这个做法的主要目的是降低计算量让更深层的训练计算量减少到基本块的5.9%，但是在浅层可能不能加快模型训练，反而减慢，因为降维升维要多开层数也会加大计算量，一定程度的加大训练难度。

官方用的ResNet

初始卷积层
    1. 卷积层
    2. BatchNorm2d
    3. ReLU
    4. 3×3 最大池化 

残差块组 1（layer1）
    2 个 Basic Block（无下采样）
    每个 Block：
    → 3×3 卷积→BN→ReLU
    → 3×3 卷积→BN
    → 残差连接（+ 输入）→ReLU

残差块组 2（layer2）
    第 1 个 Basic Block（有下采样）：
    → 3×3 卷积（步长 2）→BN→ReLU
    → 3×3 卷积→BN
    → 残差连接（输入经 1×1 卷积调整）→ReLU
    第 2 个 Basic Block（无下采样）：同上，但卷积步长 1

残差块组 3（layer3）
    第 1 个 Basic Block（有下采样）：
    → 3×3 卷积（步长 2）→BN→ReLU
    → 3×3 卷积→BN
    → 残差连接（输入经 1×1 卷积调整）→ReLU
    第 2 个 Basic Block（无下采样）：同上，卷积步长 1

残差块组 4（layer4）
    第 1 个 Basic Block（有下采样）：
    → 3×3 卷积（步长 2）→BN→ReLU
    → 3×3 卷积→BN
    → 残差连接（输入经 1×1 卷积调整）→ReLU
    第 2 个 Basic Block（无下采样）：同上，卷积步长 1

分类头
    1. 全局平均池化（AdaptiveAvgPool2d）
    2. 全连接层（Linear）

下采样：减小特征图尺寸——>降低计算量，扩大感受野


