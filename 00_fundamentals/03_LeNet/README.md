神经网络----多特征非线性多层学习

在学习代码前，我先学习了神经网络和我之前学习的模型的区别，总而言之最大的区别就是神经网络开始走的就是非线性+线性的学习了，让机器的学习更加灵活，加强了总结经验。

目前觉得神经网络了有几步特别关键特别聪明，首先由多个神经元(相当于多个模型，虽然具体实现上还是不一样)学习特征得到权值，因为学习具有一定的随机性，所以不同神经元的权值对于不同的特征的是有偏好的，这种偏好通过激活函数非线性化的激活，最后不同神经元得到了不同的相对正确的特征组合来对应预测值，把这些神经元再经过一次模型学习(输出层)，来获得最后的模型（大概是这样的，会在后续的代码学习中搞明白）


卷积->池化->卷积->池化
卷积的原理
    不同卷积核在同一张图中学习不同方向的特征(边缘，纹理，形状)
        学习方式：每次学习图片的部分区域(如5x5)的像素，可以用0填充边缘，然后定义每次滑动的像素数量
        最后加上激活函数非线性化

        C1层：卷积层 1通道 6卷积核5x5 滑动1像素 0填充 
         （提取边缘、拐角等低级特征）
            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0),  nn.Tanh(),
卷积的好处
    通过多卷积核的形式，能在多维度上捕捉图形特点，能高效的处理图形，增强了模型对图形的理解

池化的原理
    对卷积的结果求平均(减小矩阵大小)，能减少后续计算量和加强特征学习。

卷积会扩大计算量，池化过多会模糊特征，这里lenet我们只做四层处理

两个全连接层
    第一连接层 120个神经元
        在经过前卷积池化层的处理数据变成了有空间的特征集,简单说就是这个特征集不够全局化，通过这个连接层，特征全局化了
    第二连接层 84个神经元
        进一步强化特征，只保留对数字预测有用的特征

思考！！！
    学到这里的时候在想一个问题，这么多层，又没有每一层都处理损失，当然也不方便处理损失，是怎做到每一层都实现各个层的任务和目标的，后面查询得到结果就是反向传播是逐层反回的也就是说根据这个梯度的情况，每一层都会反思自己给的权重是大了还是小了然后拟合学习。

最后输出层
    10个神经元分别学10种数字的分数就行了，最后通过softmax同款概率计算方式来获得概率，最后取最大值就行了。

最后再总结一下吧，这里是卷积神经网络的雏形lenet，整个模型都在做一件事在我们前面softmax模型的基础上，提取并且学习更多特征，这个学习方式跟我们人类形似也神似，我们说盲人摸象不得全貌，最后往往得到错误的结果，根本原因就是学习的特征过少，没学过生物不知道真的神经元是不是根这里神经元一样运转，但是人在看大象时就是看各个特征，看各个特征的结合情况，摆脱了线性，灰色但是越大就越有可能是大象吗？未必，可能是鲸鱼呢，但这个时候我们加了四条腿，大象的可能性增加了，再加个鼻子，可能性又增强了。每一层，对图像的处理不同，但最终提取了特征，又抛弃了不要的特征，最后总结了特征，判断出来了结果。

